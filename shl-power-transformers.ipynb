{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-power-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWl81Y9fI1Ay"
      },
      "source": [
        "# Fit and configure scalers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuncWkC-I1A0"
      },
      "outputs": [],
      "source": [
        "# Get needed auxiliary files for colab\n",
        "!git clone https://github.com/philippmatthes/diplom\n",
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-train_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzALa8KFI1A1",
        "outputId": "ea1ad960-02e2-493d-83dd-07d6dd79077e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ]
        }
      ],
      "source": [
        "# Switch to src dir and select tensorflow\n",
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZvAw4bNI1A2",
        "outputId": "df32d7d8-5763-4fd3-c7ae-67fad9825e0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'acc_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'acc_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'acc_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_w': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True)}"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create our scalers\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "shl_dataset_X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    'gra_x', 'gra_y', 'gra_z',\n",
        "    'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "shl_dataset_y_attributes = ['labels']\n",
        "\n",
        "shl_dataset_attributes = shl_dataset_X_attributes + shl_dataset_y_attributes\n",
        "\n",
        "scalers = dict([(a, PowerTransformer()) for a in shl_dataset_X_attributes])\n",
        "scalers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twUxbLPHI1A3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-train_torso.zip'),\n",
        "    Path('shl-dataset/challenge-2019-train_bag.zip'),\n",
        "    Path('shl-dataset/challenge-2019-train_hips.zip'),\n",
        "    Path('shl-dataset/challenge-2020-train_hand.zip'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq6MXgsLI1A3",
        "outputId": "2dedc542-df06-48d5-bc8a-c2097ee0aa31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting shl-dataset/challenge-2019-train_torso.zip: 100%|██████████| 22/22 [03:06<00:00,  8.50s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:03<00:00, 18.20s/it]\n",
            "Extracting shl-dataset/challenge-2019-train_bag.zip: 100%|██████████| 22/22 [03:02<00:00,  8.29s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:04<00:00, 18.21s/it]\n",
            "Extracting shl-dataset/challenge-2019-train_hips.zip: 100%|██████████| 22/22 [03:11<00:00,  8.68s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:22<00:00, 19.10s/it]\n",
            "Extracting shl-dataset/challenge-2020-train_hand.zip: 100%|██████████| 23/23 [03:11<00:00,  8.33s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:33<00:00, 19.67s/it]\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "\n",
        "import zipfile\n",
        "import tempfile\n",
        "import pathlib\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "shl_dataset_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "    'Label.txt'\n",
        "]\n",
        "\n",
        "class SHLDataset:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def concat_inplace(self, other):\n",
        "        for attribute in shl_dataset_attributes:\n",
        "            setattr(self, attribute, np.concatenate((\n",
        "                getattr(self, attribute),\n",
        "                getattr(other, attribute)\n",
        "            ), axis=0))\n",
        "\n",
        "\n",
        "def load_shl_dataset(dataset_dir: pathlib.Path, tqdm=None, nrows=None):\n",
        "    dataset = SHLDataset()\n",
        "    if tqdm is None:\n",
        "        tqdm = lambda x, desc: x # passthrough\n",
        "    for attribute, filename in tqdm(\n",
        "        list(zip(shl_dataset_attributes, shl_dataset_files)),\n",
        "        desc=f'Loading dataset subfiles'\n",
        "    ):\n",
        "        df = pd.read_csv(dataset_dir / filename, header=None, sep=' ', nrows=nrows, dtype=np.float16)\n",
        "        np_arr = np.nan_to_num(df.to_numpy())\n",
        "        setattr(dataset, attribute, np_arr)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_zipped_shl_dataset(zip_dir: pathlib.Path, tqdm=None, nrows=None, subdir_in_zip='train'):\n",
        "    with tempfile.TemporaryDirectory() as unzip_dir:\n",
        "        with zipfile.ZipFile(zip_dir, 'r') as zip_ref:\n",
        "            if tqdm:\n",
        "                for member in tqdm(zip_ref.infolist(), desc=f'Extracting {zip_dir}'):\n",
        "                    zip_ref.extract(member, unzip_dir)\n",
        "            else:\n",
        "                zip_ref.extractall(unzip_dir)\n",
        "\n",
        "        train_dir = pathlib.Path(unzip_dir) / subdir_in_zip\n",
        "        sub_dirs = [x for x in train_dir.iterdir() if train_dir.is_dir()]\n",
        "\n",
        "        result_dataset = None\n",
        "        for sub_dir in sub_dirs:\n",
        "            sub_dataset = load_shl_dataset(train_dir / sub_dir, tqdm=tqdm, nrows=nrows)\n",
        "            if result_dataset is None:\n",
        "                result_dataset = sub_dataset\n",
        "            else:\n",
        "                result_dataset.concat_inplace(sub_dataset)\n",
        "                del sub_dataset\n",
        "        return result_dataset\n",
        "\n",
        "dataset = None\n",
        "\n",
        "for dataset_dir in DATASET_DIRS:\n",
        "    # Load dataset from zip file into temporary directory\n",
        "    partial_dataset = load_zipped_shl_dataset(dataset_dir, tqdm=tqdm)\n",
        "    if dataset is None:\n",
        "        dataset = partial_dataset\n",
        "    else:\n",
        "        dataset.concat_inplace(partial_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wluu6qhSI1A4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "export_dir = 'models/'\n",
        "num_random_samples = 10000\n",
        "\n",
        "for attribute, scaler in tqdm(scalers.items(), desc='Fitting scalers'):\n",
        "    samples = getattr(dataset, attribute)\n",
        "    random_samples_idx = np.random.choice(samples.shape[0], num_random_samples, replace=False)\n",
        "    random_samples = samples[random_samples_idx]\n",
        "    scaler.fit(random_samples.astype(np.float64))\n",
        "\n",
        "    # Platform independent export\n",
        "    transformer_params = {\n",
        "        'lambdas': list(scaler.lambdas_),\n",
        "    }\n",
        "    with open(export_dir + f'{attribute}.scaler.json', 'w') as f:\n",
        "        f.write(json.dumps(transformer_params))\n",
        "    # Python export\n",
        "    joblib.dump(scaler, export_dir + f'{attribute}.scaler.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "urZHIrHql4SL",
        "outputId": "45c02023-4947-4f54-a614-aa51c9f1971f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/diplom/src/models.zip'"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download updated model folder\n",
        "import shutil\n",
        "shutil.make_archive('models', 'zip', 'models')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "shl-power-transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
